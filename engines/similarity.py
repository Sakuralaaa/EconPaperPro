# -*- coding: utf-8 -*-
"""
ç›¸ä¼¼åº¦æ£€æµ‹æ¨¡å—
æ£€æµ‹æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œæ‰¾å‡ºç›¸ä¼¼ç‰‡æ®µ
"""

from typing import List, Tuple, Dict
from dataclasses import dataclass, field
from difflib import SequenceMatcher
import re


@dataclass
class SimilarityResult:
    """ç›¸ä¼¼åº¦æ£€æµ‹ç»“æœ"""
    overall_similarity: float  # æ•´ä½“ç›¸ä¼¼åº¦ 0-1
    char_similarity: float  # å­—ç¬¦çº§ç›¸ä¼¼åº¦
    word_similarity: float  # è¯çº§ç›¸ä¼¼åº¦
    ngram_similarity: float  # N-gramç›¸ä¼¼åº¦
    similar_segments: List[Tuple[str, str, float]] = field(default_factory=list)  # ç›¸ä¼¼ç‰‡æ®µ


class SimilarityChecker:
    """
    ç›¸ä¼¼åº¦æ£€æµ‹å™¨
    
    æ£€æµ‹ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼Œæ”¯æŒå¤šç§ç®—æ³•
    
    ä½¿ç”¨ç¤ºä¾‹:
        checker = SimilarityChecker()
        result = checker.check(text1, text2)
    """
    
    def __init__(self, ngram_size: int = 3):
        """
        åˆå§‹åŒ–ç›¸ä¼¼åº¦æ£€æµ‹å™¨
        
        Args:
            ngram_size: N-gram çš„ N å€¼
        """
        self.ngram_size = ngram_size
    
    def check(self, text1: str, text2: str) -> SimilarityResult:
        """
        æ£€æµ‹ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            SimilarityResult: ç›¸ä¼¼åº¦ç»“æœ
        """
        # è®¡ç®—å„ç±»ç›¸ä¼¼åº¦
        char_sim = self._char_similarity(text1, text2)
        word_sim = self._word_similarity(text1, text2)
        ngram_sim = self._ngram_similarity(text1, text2)
        
        # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
        overall = char_sim * 0.3 + word_sim * 0.4 + ngram_sim * 0.3
        
        # æ‰¾å‡ºç›¸ä¼¼ç‰‡æ®µ
        similar_segments = self._find_similar_segments(text1, text2)
        
        return SimilarityResult(
            overall_similarity=overall,
            char_similarity=char_sim,
            word_similarity=word_sim,
            ngram_similarity=ngram_sim,
            similar_segments=similar_segments
        )
    
    def _char_similarity(self, text1: str, text2: str) -> float:
        """
        å­—ç¬¦çº§ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            float: ç›¸ä¼¼åº¦ 0-1
        """
        return SequenceMatcher(None, text1, text2).ratio()
    
    def _word_similarity(self, text1: str, text2: str) -> float:
        """
        è¯çº§ç›¸ä¼¼åº¦ï¼ˆç®€å•åˆ†è¯ï¼‰
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            float: ç›¸ä¼¼åº¦ 0-1
        """
        # ç®€å•åˆ†è¯
        words1 = set(self._tokenize(text1))
        words2 = set(self._tokenize(text2))
        
        if not words1 or not words2:
            return 0.0
        
        # Jaccard ç›¸ä¼¼åº¦
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def _ngram_similarity(self, text1: str, text2: str) -> float:
        """
        N-gram ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            float: ç›¸ä¼¼åº¦ 0-1
        """
        ngrams1 = set(self._get_ngrams(text1))
        ngrams2 = set(self._get_ngrams(text2))
        
        if not ngrams1 or not ngrams2:
            return 0.0
        
        intersection = len(ngrams1 & ngrams2)
        union = len(ngrams1 | ngrams2)
        
        return intersection / union if union > 0 else 0.0
    
    def _tokenize(self, text: str) -> List[str]:
        """
        ç®€å•åˆ†è¯
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            List[str]: è¯åˆ—è¡¨
        """
        # ç§»é™¤æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        
        # æŒ‰ç©ºæ ¼åˆ†å‰²ï¼ˆé€‚ç”¨äºè‹±æ–‡ï¼‰
        words = text.split()
        
        # å¯¹ä¸­æ–‡è¿›è¡Œç®€å•çš„åŒå­—åˆ‡åˆ†
        chinese_words = []
        for word in words:
            if re.match(r'[\u4e00-\u9fa5]+', word):
                # ä¸­æ–‡ï¼ŒåŒå­—åˆ‡åˆ†
                for i in range(len(word) - 1):
                    chinese_words.append(word[i:i+2])
            else:
                chinese_words.append(word)
        
        return chinese_words if chinese_words else words
    
    def _get_ngrams(self, text: str) -> List[str]:
        """
        è·å– N-gram
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            List[str]: N-gram åˆ—è¡¨
        """
        # ç§»é™¤ç©ºç™½
        text = re.sub(r'\s+', '', text)
        
        if len(text) < self.ngram_size:
            return [text]
        
        ngrams = []
        for i in range(len(text) - self.ngram_size + 1):
            ngrams.append(text[i:i + self.ngram_size])
        
        return ngrams
    
    def _find_similar_segments(
        self,
        text1: str,
        text2: str,
        min_length: int = 10,
        threshold: float = 0.8
    ) -> List[Tuple[str, str, float]]:
        """
        æ‰¾å‡ºç›¸ä¼¼ç‰‡æ®µ
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            min_length: æœ€å°ç‰‡æ®µé•¿åº¦
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[Tuple[str, str, float]]: ç›¸ä¼¼ç‰‡æ®µåˆ—è¡¨ (ç‰‡æ®µ1, ç‰‡æ®µ2, ç›¸ä¼¼åº¦)
        """
        matcher = SequenceMatcher(None, text1, text2)
        similar_segments = []
        
        for match in matcher.get_matching_blocks():
            if match.size >= min_length:
                segment1 = text1[match.a:match.a + match.size]
                segment2 = text2[match.b:match.b + match.size]
                similarity = 1.0  # å®Œå…¨åŒ¹é…
                similar_segments.append((segment1, segment2, similarity))
        
        return similar_segments[:10]  # æœ€å¤šè¿”å›10ä¸ª
    
    def get_report(self, result: SimilarityResult) -> str:
        """
        ç”Ÿæˆç›¸ä¼¼åº¦æŠ¥å‘Š
        
        Args:
            result: ç›¸ä¼¼åº¦ç»“æœ
            
        Returns:
            str: Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        lines = []
        lines.append("# ğŸ“Š ç›¸ä¼¼åº¦æ£€æµ‹æŠ¥å‘Š\n")
        
        lines.append("## ç»¼åˆç›¸ä¼¼åº¦")
        lines.append(f"**{result.overall_similarity * 100:.1f}%**\n")
        
        lines.append("## å„ç»´åº¦ç›¸ä¼¼åº¦")
        lines.append(f"- å­—ç¬¦çº§ç›¸ä¼¼åº¦ï¼š{result.char_similarity * 100:.1f}%")
        lines.append(f"- è¯çº§ç›¸ä¼¼åº¦ï¼š{result.word_similarity * 100:.1f}%")
        lines.append(f"- N-gramç›¸ä¼¼åº¦ï¼š{result.ngram_similarity * 100:.1f}%\n")
        
        if result.similar_segments:
            lines.append("## ç›¸ä¼¼ç‰‡æ®µ")
            for i, (seg1, seg2, sim) in enumerate(result.similar_segments[:5], 1):
                lines.append(f"### ç‰‡æ®µ {i} (ç›¸ä¼¼åº¦: {sim * 100:.0f}%)")
                lines.append(f"```\n{seg1[:100]}...\n```")
        
        return "\n".join(lines)
    
    def check_against_corpus(
        self,
        text: str,
        corpus: List[str]
    ) -> Dict[int, float]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸è¯­æ–™åº“çš„ç›¸ä¼¼åº¦
        
        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            corpus: è¯­æ–™åº“
            
        Returns:
            Dict[int, float]: ä¸å„æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        """
        results = {}
        
        for i, doc in enumerate(corpus):
            result = self.check(text, doc)
            results[i] = result.overall_similarity
        
        return results
