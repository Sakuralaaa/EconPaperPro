# -*- coding: utf-8 -*-
"""
é™AIå¼•æ“æ¨¡å—
æ¶ˆé™¤AIå†™ä½œç—•è¿¹ï¼Œä½¿æ–‡æœ¬æ›´å…·äººç±»å­¦è€…é£æ ¼
"""

from typing import List, Dict, Optional
from dataclasses import dataclass, field
from core.llm import get_llm_client
from core.prompts import PromptTemplates


def split_sentences(text: str) -> List[str]:
    """
    åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­åˆ—è¡¨
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        List[str]: å¥å­åˆ—è¡¨
    """
    # æ ‡å‡†åŒ–å¥æœ«æ ‡ç‚¹
    normalized = text.replace("ï¼", "ã€‚").replace("ï¼Ÿ", "ã€‚")
    sentences = [s.strip() for s in normalized.split("ã€‚") if s.strip()]
    return sentences


@dataclass
class AIDetectionResult:
    """AIæ£€æµ‹ç»“æœ"""
    overall_score: float  # AIæ¦‚ç‡ 0-100
    dimensions: Dict[str, float]  # å„ç»´åº¦è¯„åˆ†
    ai_markers: List[str]  # AIç—•è¿¹ç¤ºä¾‹
    suggestions: List[str]  # æ”¹è¿›å»ºè®®


@dataclass
class DeAIResult:
    """é™AIå¤„ç†ç»“æœ"""
    original: str
    processed: str
    ai_score_before: float
    ai_score_after: float
    changes: List[str]


class DeAIEngine:
    """
    é™AIå¼•æ“
    
    æ¶ˆé™¤AIå†™ä½œç—•è¿¹ï¼Œä½¿æ–‡æœ¬æ›´å…·äººç±»å­¦è€…é£æ ¼
    
    ä½¿ç”¨ç¤ºä¾‹:
        engine = DeAIEngine()
        result = engine.process(content)
    """
    
    # AIå†™ä½œç‰¹å¾
    AI_MARKERS = {
        # ç»“æ„åŒ–åºåˆ—
        "sequence_markers": [
            "é¦–å…ˆ", "å…¶æ¬¡", "å†æ¬¡", "æœ€å", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "ç¬¬å››",
            "ä¸€æ–¹é¢", "å¦ä¸€æ–¹é¢", "æ­¤å¤–", "åŒæ—¶", "å¦å¤–", "ä¸æ­¤åŒæ—¶",
            "ç´§æ¥ç€", "éšå", "è¿›ä¸€æ­¥",
        ],
        # å¡«å……çŸ­è¯­
        "filler_phrases": [
            "å€¼å¾—æ³¨æ„çš„æ˜¯", "éœ€è¦æŒ‡å‡ºçš„æ˜¯", "ç»¼ä¸Šæ‰€è¿°", "æ€»çš„æ¥è¯´", 
            "æ€»è€Œè¨€ä¹‹", "ä¸éš¾å‘ç°", "æ˜¾è€Œæ˜“è§", "æ¯‹åº¸ç½®ç–‘", "ä¸å¯å¦è®¤", 
            "ä¼—æ‰€å‘¨çŸ¥", "äº‹å®ä¸Š", "å®é™…ä¸Š", "å¯ä»¥è¯´", "ç”±æ­¤å¯è§",
            "éœ€è¦å¼ºè°ƒçš„æ˜¯", "ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯", "ä¸è¨€è€Œå–»",
        ],
        # æ¨¡ç³Šè¡¨è¾¾
        "vague_expressions": [
            "åœ¨ä¸€å®šç¨‹åº¦ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š", "ä»æŸç§è§’åº¦æ¥çœ‹",
            "å¯èƒ½ä¼š", "æˆ–è®¸", "å¤§æ¦‚", "ä¼¼ä¹", "è²Œä¼¼",
            "ç›¸å¯¹è€Œè¨€", "æ€»ä½“ä¸Šçœ‹", "ä¸€èˆ¬æ¥è¯´", "é€šå¸¸æƒ…å†µä¸‹",
        ],
        # è¿‡åº¦æ­£å¼
        "overly_formal": [
            "é‰´äºæ­¤", "åŸºäºæ­¤", "æ®æ­¤", "ç”±æ­¤å¯è§", "ç”±æ­¤å¯çŸ¥", 
            "ç”±ä¸Šå¯çŸ¥", "ç»¼åˆä»¥ä¸Šåˆ†æ", "åŸºäºä¸Šè¿°åˆ†æ",
            "æ‰¿ä¸Šæ‰€è¿°", "å¦‚å‰æ‰€è¿°", "æ­£å¦‚å‰æ–‡æ‰€è¿°",
        ],
        # è¿æ¥è¯æ»¥ç”¨
        "connector_abuse": [
            "ç„¶è€Œ", "ä½†æ˜¯", "å› æ­¤", "æ‰€ä»¥", "æ•…è€Œ", "äºæ˜¯",
            "å°½ç®¡å¦‚æ­¤", "è™½ç„¶å¦‚æ­¤", "å³ä¾¿å¦‚æ­¤",
        ]
    }
    
    def __init__(self):
        """åˆå§‹åŒ–é™AIå¼•æ“"""
        self._llm = None
    
    @property
    def llm(self):
        """å»¶è¿ŸåŠ è½½LLMå®¢æˆ·ç«¯"""
        if self._llm is None:
            self._llm = get_llm_client()
        return self._llm
    
    def process(self, content: str) -> DeAIResult:
        """
        æ‰§è¡Œé™AIå¤„ç†
        
        Args:
            content: åŸå§‹æ–‡æœ¬
            
        Returns:
            DeAIResult: é™AIç»“æœ
        """
        # 1. æ£€æµ‹AIç—•è¿¹
        ai_score_before = self.estimate_ai_score(content)
        
        # 2. äººæ€§åŒ–æ”¹å†™
        processed = self._humanize(content)
        
        # 3. å†æ¬¡æ£€æµ‹
        ai_score_after = self.estimate_ai_score(processed)
        
        # 4. è¯†åˆ«å˜åŒ–
        changes = self._identify_changes(content, processed)
        
        return DeAIResult(
            original=content,
            processed=processed,
            ai_score_before=ai_score_before,
            ai_score_after=ai_score_after,
            changes=changes
        )
    
    def estimate_ai_score(self, text: str) -> float:
        """
        ä¼°ç®—AIå†™ä½œæ¦‚ç‡
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            float: AIæ¦‚ç‡ (0-100)
        """
        if not text:
            return 0.0
        
        # å¯¹äºéå¸¸çŸ­çš„æ–‡æœ¬ï¼ŒAIæ£€æµ‹ä¸å¯é 
        if len(text) < 30:
            return 0.0
        
        scores = []
        weights = []
        
        # 1. æ£€æµ‹ç»“æ„åŒ–åºåˆ—ï¼ˆæƒé‡20%ï¼‰
        sequence_count = sum(
            text.count(m) for m in self.AI_MARKERS["sequence_markers"]
        )
        # æ ¹æ®æ–‡æœ¬é•¿åº¦å½’ä¸€åŒ–
        text_len_factor = len(text) / 1000
        normalized_seq = sequence_count / max(1, text_len_factor)
        sequence_score = min(100, normalized_seq * 12)
        scores.append(sequence_score)
        weights.append(0.20)
        
        # 2. æ£€æµ‹å¡«å……çŸ­è¯­ï¼ˆæƒé‡25%ï¼‰
        filler_count = sum(
            text.count(p) for p in self.AI_MARKERS["filler_phrases"]
        )
        normalized_filler = filler_count / max(1, text_len_factor)
        filler_score = min(100, normalized_filler * 20)
        scores.append(filler_score)
        weights.append(0.25)
        
        # 3. æ£€æµ‹æ¨¡ç³Šè¡¨è¾¾ï¼ˆæƒé‡15%ï¼‰
        vague_count = sum(
            text.count(e) for e in self.AI_MARKERS["vague_expressions"]
        )
        normalized_vague = vague_count / max(1, text_len_factor)
        vague_score = min(100, normalized_vague * 15)
        scores.append(vague_score)
        weights.append(0.15)
        
        # 4. æ£€æµ‹å¥å­é•¿åº¦å‡åŒ€åº¦ï¼ˆæƒé‡20%ï¼‰
        sentences = split_sentences(text)
        if len(sentences) >= 3:
            lengths = [len(s) for s in sentences if len(s) > 5]
            if lengths:
                avg_len = sum(lengths) / len(lengths)
                variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
                std_dev = variance ** 0.5
                # æ ‡å‡†å·®è¶Šå°ï¼Œè¶Šå¯èƒ½æ˜¯AIï¼ˆå¥å­é•¿åº¦è¿‡äºå‡åŒ€ï¼‰
                # äººç±»å†™ä½œæ ‡å‡†å·®é€šå¸¸åœ¨15-40ä¹‹é—´
                if std_dev < 10:
                    uniformity_score = 90
                elif std_dev < 20:
                    uniformity_score = 60
                elif std_dev < 30:
                    uniformity_score = 30
                else:
                    uniformity_score = 10
                scores.append(uniformity_score)
                weights.append(0.20)
        
        # 5. è¿‡åº¦æ­£å¼è¡¨è¾¾ï¼ˆæƒé‡10%ï¼‰
        formal_count = sum(
            text.count(f) for f in self.AI_MARKERS["overly_formal"]
        )
        normalized_formal = formal_count / max(1, text_len_factor)
        formal_score = min(100, normalized_formal * 18)
        scores.append(formal_score)
        weights.append(0.10)
        
        # 6. è¿æ¥è¯æ»¥ç”¨ï¼ˆæƒé‡10%ï¼‰
        connector_count = sum(
            text.count(c) for c in self.AI_MARKERS["connector_abuse"]
        )
        normalized_connector = connector_count / max(1, text_len_factor)
        connector_score = min(100, normalized_connector * 8)
        scores.append(connector_score)
        weights.append(0.10)
        
        # è®¡ç®—åŠ æƒå¹³å‡
        if scores and weights:
            total_weight = sum(weights)
            weighted_sum = sum(s * w for s, w in zip(scores, weights))
            return weighted_sum / total_weight
        
        return 50.0
    
    def detect_ai_features(self, text: str) -> AIDetectionResult:
        """
        è¯¦ç»†æ£€æµ‹AIç‰¹å¾
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            AIDetectionResult: æ£€æµ‹ç»“æœ
        """
        dimensions = {}
        ai_markers = []
        suggestions = []
        
        # æ£€æµ‹å„ç»´åº¦
        for category, markers in self.AI_MARKERS.items():
            found = [m for m in markers if m in text]
            count = len(found)
            
            if category == "sequence_markers":
                dimensions["ç»“æ„è§„æ•´åº¦"] = min(10, count * 2)
                if found:
                    ai_markers.extend([f"åºåˆ—è¯: {m}" for m in found[:3]])
                    suggestions.append("å‡å°‘ä½¿ç”¨'é¦–å…ˆã€å…¶æ¬¡ã€æœ€å'ç­‰åºåˆ—è¯")
                    
            elif category == "filler_phrases":
                dimensions["å¡«å……çŸ­è¯­"] = min(10, count * 3)
                if found:
                    ai_markers.extend([f"å¡«å……è¯­: {m}" for m in found[:3]])
                    suggestions.append("åˆ é™¤'å€¼å¾—æ³¨æ„çš„æ˜¯'ç­‰å¡«å……æ€§çŸ­è¯­")
                    
            elif category == "vague_expressions":
                dimensions["æ¨¡ç³Šè¡¨è¾¾"] = min(10, count * 2)
                if found:
                    ai_markers.extend([f"æ¨¡ç³Šè¯­: {m}" for m in found[:3]])
                    suggestions.append("ä½¿ç”¨æ›´å…·ä½“çš„è¡¨è¿°æ›¿ä»£æ¨¡ç³Šè¡¨è¾¾")
                    
            elif category == "overly_formal":
                dimensions["è¿‡åº¦æ­£å¼"] = min(10, count * 3)
                if found:
                    ai_markers.extend([f"æ­£å¼è¯­: {m}" for m in found[:3]])
                    suggestions.append("é€‚å½“é™ä½è¯­è¨€çš„æ­£å¼ç¨‹åº¦")
        
        # æ£€æµ‹å¥å­å‡åŒ€åº¦
        sentences = split_sentences(text)
        if len(sentences) >= 3:
            lengths = [len(s) for s in sentences]
            avg_len = sum(lengths) / len(lengths)
            variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
            uniformity = max(0, 10 - variance / 50)
            dimensions["å¥å¼å‡åŒ€åº¦"] = uniformity
            if uniformity > 7:
                suggestions.append("å˜åŒ–å¥å­é•¿åº¦ï¼Œæ‰“ç ´å‡åŒ€èŠ‚å¥")
        
        overall = sum(dimensions.values()) / len(dimensions) * 10 if dimensions else 50
        
        return AIDetectionResult(
            overall_score=overall,
            dimensions=dimensions,
            ai_markers=ai_markers,
            suggestions=suggestions
        )
    
    def _humanize(self, content: str) -> str:
        """
        äººæ€§åŒ–æ”¹å†™
        
        Args:
            content: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ”¹å†™åçš„æ–‡æœ¬
        """
        prompt = PromptTemplates.DEAI_HUMANIZE.format(content=content)
        
        try:
            processed = self.llm.invoke(
                prompt,
                system_prompt="ä½ æ˜¯èµ„æ·±å­¦æœ¯å†™ä½œä¸“å®¶ï¼Œè¯·å°†AIé£æ ¼çš„æ–‡æœ¬æ”¹å†™ä¸ºæ›´å…·äººç±»å­¦è€…ç‰¹è‰²çš„è¡¨è¾¾ã€‚"
            )
            return processed
            
        except Exception:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œå°è¯•è§„åˆ™æ›¿æ¢
            return self._rule_based_humanize(content)
    
    def _rule_based_humanize(self, content: str) -> str:
        """
        åŸºäºè§„åˆ™çš„äººæ€§åŒ–æ”¹å†™
        
        Args:
            content: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ”¹å†™åçš„æ–‡æœ¬
        """
        result = content
        
        # æ›¿æ¢å¡«å……çŸ­è¯­
        filler_replacements = {
            "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ": "",
            "éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œ": "",
            "ç»¼ä¸Šæ‰€è¿°ï¼Œ": "",
            "æ€»çš„æ¥è¯´ï¼Œ": "",
            "ä¸éš¾å‘ç°ï¼Œ": "",
            "æ˜¾è€Œæ˜“è§ï¼Œ": "",
            "æ¯‹åº¸ç½®ç–‘ï¼Œ": "",
            "ä¼—æ‰€å‘¨çŸ¥ï¼Œ": "",
            "äº‹å®ä¸Šï¼Œ": "",
            "ä¸å¯å¦è®¤ï¼Œ": "",
            "éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œ": "",
            "ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯ï¼Œ": "",
        }
        
        for old, new in filler_replacements.items():
            result = result.replace(old, new)
        
        # æ›¿æ¢è¿‡äºè§„æ•´çš„åºåˆ—ç»“æ„
        # åªæ›¿æ¢æ¯ä¸ªæ ‡è®°çš„ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œé¿å…è¿‡åº¦ä¿®æ”¹
        sequence_replacements = {
            "é¦–å…ˆï¼Œ": "",  # åˆ é™¤"é¦–å…ˆ"ä½¿ç»“æ„ä¸é‚£ä¹ˆæœºæ¢°
            "å…¶æ¬¡ï¼Œ": "åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ",
            "å†æ¬¡ï¼Œ": "åŒæ ·å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œ",
            "æœ€åï¼Œ": "æ›´é‡è¦çš„æ˜¯ï¼Œ",
            "ä¸€æ–¹é¢ï¼Œ": "ä»ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œ",
            "å¦ä¸€æ–¹é¢ï¼Œ": "ä»å¦ä¸€ä¸ªç»´åº¦æ¥çœ‹ï¼Œ",
        }
        
        for old, new in sequence_replacements.items():
            result = result.replace(old, new, 1)  # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°
        
        # æ›¿æ¢è¿‡åº¦æ­£å¼çš„è¡¨è¾¾
        formal_replacements = {
            "é‰´äºæ­¤ï¼Œ": "åŸºäºè¿™ä¸€è€ƒè™‘ï¼Œ",
            "åŸºäºæ­¤ï¼Œ": "ç”±æ­¤ï¼Œ",
            "ç»¼åˆä»¥ä¸Šåˆ†æï¼Œ": "ä»ä¸Šè¿°åˆ†ææ¥çœ‹ï¼Œ",
            "ç”±æ­¤å¯è§ï¼Œ": "è¿™è¡¨æ˜ï¼Œ",
            "ç”±æ­¤å¯çŸ¥ï¼Œ": "å¯ä»¥çœ‹å‡ºï¼Œ",
        }
        
        for old, new in formal_replacements.items():
            result = result.replace(old, new)
        
        return result
    
    def _identify_changes(self, original: str, processed: str) -> List[str]:
        """
        è¯†åˆ«ä¸»è¦å˜åŒ–
        
        Args:
            original: åŸå§‹æ–‡æœ¬
            processed: å¤„ç†åæ–‡æœ¬
            
        Returns:
            List[str]: å˜åŒ–æè¿°
        """
        changes = []
        
        # æ£€æµ‹åºåˆ—è¯å˜åŒ–
        orig_seq = sum(original.count(m) for m in self.AI_MARKERS["sequence_markers"])
        proc_seq = sum(processed.count(m) for m in self.AI_MARKERS["sequence_markers"])
        if proc_seq < orig_seq:
            changes.append("å‡å°‘äº†åºåˆ—æ€§è¯æ±‡ä½¿ç”¨")
        
        # æ£€æµ‹å¡«å……çŸ­è¯­å˜åŒ–
        orig_fill = sum(original.count(p) for p in self.AI_MARKERS["filler_phrases"])
        proc_fill = sum(processed.count(p) for p in self.AI_MARKERS["filler_phrases"])
        if proc_fill < orig_fill:
            changes.append("åˆ é™¤äº†å¡«å……æ€§çŸ­è¯­")
        
        # é•¿åº¦å˜åŒ–
        if len(processed) < len(original) * 0.95:
            changes.append("ç²¾ç®€äº†å†—ä½™è¡¨è¾¾")
        
        if not changes:
            changes.append("è°ƒæ•´äº†è¡¨è¾¾æ–¹å¼")
        
        return changes
    
    def get_report(self, result: DeAIResult) -> str:
        """
        ç”Ÿæˆé™AIæŠ¥å‘Š
        
        Args:
            result: é™AIç»“æœ
            
        Returns:
            str: Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        lines = []
        lines.append("# ğŸ¤– é™AIå¤„ç†æŠ¥å‘Š\n")
        
        # AIæ¦‚ç‡å˜åŒ–
        reduction = result.ai_score_before - result.ai_score_after
        lines.append("## AIæ¦‚ç‡å˜åŒ–")
        lines.append(f"- å¤„ç†å‰AIæ¦‚ç‡ï¼š{result.ai_score_before:.1f}%")
        lines.append(f"- å¤„ç†åAIæ¦‚ç‡ï¼š{result.ai_score_after:.1f}%")
        lines.append(f"- é™ä½å¹…åº¦ï¼š**{reduction:.1f}%**\n")
        
        # ä¸»è¦å˜åŒ–
        lines.append("## ä¸»è¦å˜åŒ–")
        for change in result.changes:
            lines.append(f"- {change}")
        
        return "\n".join(lines)
