# -*- coding: utf-8 -*-
"""
é™AIå¼•æ“æ¨¡å— (ä¼˜åŒ–ç‰ˆ)
æ¶ˆé™¤AIå†™ä½œç—•è¿¹ï¼Œä½¿æ–‡æœ¬æ›´å…·äººç±»å­¦è€…é£æ ¼

ä¼˜åŒ–å†…å®¹ï¼š
1. ä½¿ç”¨ä¸“ä¸šçš„è®ºæ–‡ä¿®æ”¹åŠ©æ‰‹æç¤ºè¯
2. å¢å¼ºè§„åˆ™æ›¿æ¢ç­–ç•¥
3. åˆ†å¥ç²¾ç»†å¤„ç†
4. å¤šç»´åº¦AIæ£€æµ‹
"""

from typing import List, Dict, Optional
from dataclasses import dataclass, field
import re
import random
from core.llm import get_llm_client


def split_sentences(text: str) -> List[str]:
    """åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­åˆ—è¡¨"""
    pattern = r'([ã€‚ï¼ï¼Ÿï¼›.!?;])'
    parts = re.split(pattern, text)
    
    sentences = []
    i = 0
    while i < len(parts):
        if i + 1 < len(parts) and re.match(pattern, parts[i + 1]):
            sentences.append(parts[i] + parts[i + 1])
            i += 2
        else:
            if parts[i].strip():
                sentences.append(parts[i])
            i += 1
    
    return sentences


@dataclass
class AIDetectionResult:
    """AIæ£€æµ‹ç»“æœ"""
    overall_score: float  # AIæ¦‚ç‡ 0-100
    dimensions: Dict[str, float]  # å„ç»´åº¦è¯„åˆ†
    ai_markers: List[str]  # AIç—•è¿¹ç¤ºä¾‹
    suggestions: List[str]  # æ”¹è¿›å»ºè®®


@dataclass
class DeAIResult:
    """é™AIå¤„ç†ç»“æœ"""
    original: str
    processed: str
    ai_score_before: float
    ai_score_after: float
    changes: List[str]
    sentences_processed: int = 0


class DeAIEngine:
    """
    é™AIå¼•æ“ (ä¼˜åŒ–ç‰ˆ)
    
    ä½¿ç”¨ä¸“ä¸šçš„è®ºæ–‡ä¿®æ”¹åŠ©æ‰‹ç­–ç•¥ï¼š
    1. è§£é‡Šæ€§æ‰©å±• - ä½¿è¡¨è¾¾æ›´è¯¦å°½
    2. ç³»ç»Ÿæ€§è¯æ±‡æ›¿æ¢ - å›ºå®šæ›¿æ¢æ¨¡å¼
    3. å¥å¼å¾®è°ƒ - å¢å¼ºé€»è¾‘è¿æ¥
    4. æ¶ˆé™¤AIå…¸å‹ç‰¹å¾ - åˆ é™¤å¡«å……è¯/è§„æ•´ç»“æ„
    """
    
    # AIå†™ä½œç‰¹å¾ (ç”¨äºæ£€æµ‹)
    AI_MARKERS = {
        "sequence_markers": [
            "é¦–å…ˆ", "å…¶æ¬¡", "å†æ¬¡", "æœ€å", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "ç¬¬å››",
            "ä¸€æ–¹é¢", "å¦ä¸€æ–¹é¢", "æ­¤å¤–", "åŒæ—¶", "å¦å¤–", "ä¸æ­¤åŒæ—¶",
            "ç´§æ¥ç€", "éšå", "è¿›ä¸€æ­¥",
        ],
        "filler_phrases": [
            "å€¼å¾—æ³¨æ„çš„æ˜¯", "éœ€è¦æŒ‡å‡ºçš„æ˜¯", "ç»¼ä¸Šæ‰€è¿°", "æ€»çš„æ¥è¯´", 
            "æ€»è€Œè¨€ä¹‹", "ä¸éš¾å‘ç°", "æ˜¾è€Œæ˜“è§", "æ¯‹åº¸ç½®ç–‘", "ä¸å¯å¦è®¤", 
            "ä¼—æ‰€å‘¨çŸ¥", "äº‹å®ä¸Š", "å®é™…ä¸Š", "å¯ä»¥è¯´", "ç”±æ­¤å¯è§",
            "éœ€è¦å¼ºè°ƒçš„æ˜¯", "ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯", "ä¸è¨€è€Œå–»",
        ],
        "vague_expressions": [
            "åœ¨ä¸€å®šç¨‹åº¦ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š", "ä»æŸç§è§’åº¦æ¥çœ‹",
            "å¯èƒ½ä¼š", "æˆ–è®¸", "å¤§æ¦‚", "ä¼¼ä¹", "è²Œä¼¼",
            "ç›¸å¯¹è€Œè¨€", "æ€»ä½“ä¸Šçœ‹", "ä¸€èˆ¬æ¥è¯´", "é€šå¸¸æƒ…å†µä¸‹",
        ],
        "overly_formal": [
            "é‰´äºæ­¤", "åŸºäºæ­¤", "æ®æ­¤", "ç”±æ­¤å¯è§", "ç”±æ­¤å¯çŸ¥", 
            "ç”±ä¸Šå¯çŸ¥", "ç»¼åˆä»¥ä¸Šåˆ†æ", "åŸºäºä¸Šè¿°åˆ†æ",
            "æ‰¿ä¸Šæ‰€è¿°", "å¦‚å‰æ‰€è¿°", "æ­£å¦‚å‰æ–‡æ‰€è¿°",
        ],
        "connector_abuse": [
            "ç„¶è€Œ", "ä½†æ˜¯", "å› æ­¤", "æ‰€ä»¥", "æ•…è€Œ", "äºæ˜¯",
            "å°½ç®¡å¦‚æ­¤", "è™½ç„¶å¦‚æ­¤", "å³ä¾¿å¦‚æ­¤",
        ]
    }
    
    # ä¸“ä¸šè¯æ±‡æ›¿æ¢è§„åˆ™ (åŸºäºç”¨æˆ·æä¾›çš„ç­–ç•¥)
    WORD_SUBSTITUTIONS = {
        # åŠ¨è¯æ›¿æ¢
        "é‡‡ç”¨": ["è¿ç”¨", "é€‰ç”¨"],
        "ä½¿ç”¨": ["è¿ç”¨", "å€ŸåŠ©"],
        "åŸºäº": ["ä¾æ®", "æ ¹æ®"],
        "åˆ©ç”¨": ["å€ŸåŠ©", "è¿ç”¨"],
        "é€šè¿‡": ["å€ŸåŠ©", "ç»ç”±"],
        "å¹¶": ["å¹¶ä¸”", "åŒæ—¶"],
        # åè¯/å½¢å®¹è¯æ›¿æ¢
        "åŸå› ": ["ç¼˜ç”±"],
        "ç¬¦åˆ": ["å¥‘åˆ"],
        "é€‚åˆ": ["é€‚å®œ"],
        "ç‰¹ç‚¹": ["ç‰¹æ€§"],
        "æå‡": ["æé«˜"],
        "æé«˜": ["æå‡"],
        "æå¤§åœ°": ["åœ¨æå¤§ç¨‹åº¦ä¸Š"],
        "ç«‹å³": ["å³åˆ»"],
        # è¿è¯æ›¿æ¢
        "å’Œ": ["ä»¥åŠ", "ä¸"],
        "åŠ": ["ä»¥åŠ"],
        "ä¸": ["ä»¥åŠ", "åŒ"],
        "å…¶": ["è¯¥", "ç›¸åº”"],
    }
    
    # åŠ¨è¯æ‰©å±•è§„åˆ™
    VERB_EXPANSIONS = {
        "ç®¡ç†": ["å¼€å±•ç®¡ç†å·¥ä½œ", "è¿›è¡Œç®¡ç†"],
        "é…ç½®": ["è¿›è¡Œé…ç½®", "å®Œæˆé…ç½®æ“ä½œ"],
        "å¤„ç†": ["æ‰§è¡Œå¤„ç†å·¥ä½œ", "è¿›è¡Œå¤„ç†"],
        "æ¢å¤": ["æ‰§è¡Œæ¢å¤æ“ä½œ"],
        "å®ç°": ["å¾—ä»¥å®ç°", "ç”¨ä»¥å®ç°"],
        "äº¤äº’": ["è¿›è¡Œæ•°æ®äº¤äº’", "å¼€å±•äº¤äº’"],
        "åˆ†æ": ["å¼€å±•åˆ†æå·¥ä½œ", "è¿›è¡Œåˆ†æ"],
        "ç ”ç©¶": ["å¼€å±•ç ”ç©¶å·¥ä½œ", "è¿›è¡Œæ·±å…¥ç ”ç©¶"],
        "æ¢è®¨": ["å±•å¼€æ¢è®¨", "è¿›è¡Œæ¢è®¨"],
        "éªŒè¯": ["è¿›è¡ŒéªŒè¯", "å¼€å±•éªŒè¯å·¥ä½œ"],
        "æ£€éªŒ": ["è¿›è¡Œæ£€éªŒ", "å¼€å±•æ£€éªŒ"],
        "ä¼˜åŒ–": ["è¿›è¡Œä¼˜åŒ–", "å¼€å±•ä¼˜åŒ–å·¥ä½œ"],
    }
    
    # å¡«å……çŸ­è¯­åˆ é™¤æ˜ å°„
    FILLER_REPLACEMENTS = {
        "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ": "",
        "éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œ": "",
        "ç»¼ä¸Šæ‰€è¿°ï¼Œ": "",
        "æ€»çš„æ¥è¯´ï¼Œ": "",
        "ä¸éš¾å‘ç°ï¼Œ": "",
        "æ˜¾è€Œæ˜“è§ï¼Œ": "",
        "æ¯‹åº¸ç½®ç–‘ï¼Œ": "",
        "ä¼—æ‰€å‘¨çŸ¥ï¼Œ": "",
        "äº‹å®ä¸Šï¼Œ": "",
        "ä¸å¯å¦è®¤ï¼Œ": "",
        "éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œ": "",
        "ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯ï¼Œ": "",
        "æ€»è€Œè¨€ä¹‹ï¼Œ": "",
        "ç”±æ­¤å¯è§ï¼Œ": "è¿™è¡¨æ˜ï¼Œ",
        "åŸºäºæ­¤ï¼Œ": "æ®æ­¤ï¼Œ",
        "é‰´äºæ­¤ï¼Œ": "è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œ",
    }
    
    # å¥å¼è½¬æ¢æ¨¡å¼
    SENTENCE_TRANSFORMS = [
        # (åŸå§‹æ¨¡å¼, æ›¿æ¢æ¨¡å¼)
        (r"è‹¥(.+?)ï¼Œåˆ™(.+)", r"å¦‚æœ\1ï¼Œé‚£ä¹ˆ\2"),
        (r"(.+?)å¯¹(.+?)äº§ç”Ÿäº†(.+?)å½±å“", r"\1å¯¹\2å½¢æˆäº†\3ä½œç”¨"),
        (r"^é¦–å…ˆï¼Œ(.+?)ã€‚å…¶æ¬¡ï¼Œ(.+?)ã€‚æœ€åï¼Œ(.+?)ã€‚", r"\1ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ\2ã€‚æ›´ä¸ºé‡è¦çš„æ˜¯ï¼Œ\3ã€‚"),
        (r"ä¸€æ–¹é¢ï¼Œ(.+?)ï¼›å¦ä¸€æ–¹é¢ï¼Œ(.+?)", r"\1ã€‚ä¸æ­¤åŒæ—¶ï¼Œ\2"),
    ]
    
    # ä¸“ä¸šè®ºæ–‡ä¿®æ”¹åŠ©æ‰‹ç³»ç»Ÿæç¤ºè¯
    SYSTEM_PROMPT = """ä½ çš„è§’è‰²ä¸ç›®æ ‡ï¼š

ä½ ç°åœ¨æ‰®æ¼”ä¸€ä¸ªä¸“ä¸šçš„"è®ºæ–‡ï¼ˆæˆ–æŠ€æœ¯æ–‡æ¡£ï¼‰ä¿®æ”¹åŠ©æ‰‹"ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ¥æ”¶ä¸€æ®µä¸­æ–‡åŸæ–‡ï¼ˆé€šå¸¸æ˜¯æŠ€æœ¯æ€§æˆ–å­¦æœ¯æ€§çš„æè¿°ï¼‰ï¼Œå¹¶å°†å…¶æ”¹å†™æˆä¸€ç§ç‰¹å®šçš„é£æ ¼ã€‚

è¿™ç§é£æ ¼çš„ç‰¹ç‚¹æ˜¯ï¼šåœ¨ä¿æŒä¸“ä¸šæ€§çš„å‰æä¸‹ï¼Œå¢å¼ºæ–‡æœ¬çš„è§£é‡Šæ€§å’Œå¯è¯»æ€§ï¼Œä½¿è¡¨è¾¾æ›´ä¸ºè¯¦å°½å’Œæµç•…ã€‚æœ€ç»ˆè¾“å‡ºåº”æ˜¯ä¸€ç¯‡ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€è¯­è¨€ç²¾ç»ƒçš„å­¦æœ¯æ€§æ–‡æœ¬ã€‚ä½ çš„ç›®æ ‡æ˜¯ç²¾ç¡®åœ°æ¨¡ä»¿åˆ†æå¾—å‡ºçš„ä¿®æ”¹æ¨¡å¼ï¼Œç”Ÿæˆ"ä¿®æ”¹å"é£æ ¼çš„æ–‡æœ¬ï¼ŒåŒæ—¶åŠ¡å¿…ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæŠ€æœ¯ä¿¡æ¯ã€é€»è¾‘å…³ç³»å’Œäº‹å®å‡†ç¡®æ€§ã€‚

æ ¸å¿ƒåŸåˆ™ä¸æ–°å¢è¦æ±‚ï¼ˆå…¨å±€æœ€é«˜ä¼˜å…ˆçº§ï¼‰

åšå®ˆå­¦æœ¯ä¸¥è°¨æ€§ï¼š
- ç»å¯¹ä¿ç•™ä¸“æœ‰åè¯ï¼šä»»ä½•å­¦æœ¯æ¦‚å¿µã€æŠ€æœ¯æœ¯è¯­ã€ä»£ç æ ‡è¯†ç¬¦ã€åº“åã€é…ç½®é¡¹ç­‰ä¸“æœ‰å†…å®¹å¿…é¡»ä¿æŒåŸæ ·ï¼Œä¸å¾—è¿›è¡Œä»»ä½•å½¢å¼çš„ä¿®æ”¹æˆ–è½¬å†™ã€‚
- é¿å…ç©ºæ³›ä¿®é¥°ï¼šé™¤éåŸæ–‡å·²æœ‰ï¼Œå¦åˆ™ä¸å¾—å¼•å…¥"ç³»ç»Ÿæ€§"ã€"æ ¹æœ¬æ€§"ã€"æœ¬è´¨ä¸Š"ç­‰æ„ä¹‰å®½æ³›ã€ç¼ºä¹å…·ä½“æŒ‡å‘çš„"é«˜ç«¯"è¯æ±‡ï¼Œç¡®ä¿è¯­è¨€çš„ç²¾ç¡®åº¦å’Œå®¢è§‚æ€§ã€‚

å¼ºåŒ–å¥å­ç»“æ„ä¸è¿è´¯æ€§ï¼š
- ä¼˜å…ˆä½¿ç”¨å®Œæ•´å¥å¼ï¼šè‡´åŠ›äºè¾“å‡ºç»“æ„å®Œæ•´çš„é•¿å¥ï¼Œå‡å°‘ä½¿ç”¨é€—å·ã€ç ´æŠ˜å·å°†å¥å­åˆ†å‰²æˆé›¶æ•£çŸ­è¯­çš„æƒ…å†µã€‚å¯ä»¥é€šè¿‡ä½¿ç”¨è¿è¯æˆ–è°ƒæ•´è¯­åºï¼Œå°†åŸæœ¬åˆ†æ•£çš„ä¿¡æ¯ç‚¹æœ‰æœºåœ°ç»„ç»‡åœ¨ä¸€èµ·ã€‚
- ç¡®ä¿è¡Œæ–‡æµç•…ï¼šæ”¹å†™åº”ä»¥æå‡æ–‡æœ¬çš„æµç•…åº¦ä¸ºç›®æ ‡ï¼Œé¿å…å› ç”Ÿç¡¬å¥—ç”¨è§„åˆ™è€Œå¯¼è‡´è¯­å¥ç»“æ„ä¸è‡ªç„¶æˆ–é€»è¾‘æ–­è£‚ã€‚

æ§åˆ¶è¾“å‡ºç¯‡å¹…ï¼š
- ç¯‡å¹…ä¸¥æ ¼å¯¹ç­‰ï¼šä¿®æ”¹åçš„æ–‡æœ¬é•¿åº¦åº”ä¸åŸæ–‡å¤§è‡´ç›¸ç­‰ã€‚æ–°å¢çš„è§£é‡Šæ€§è¯è¯­æ˜¯ä¸ºäº†ä½¿è¯­ä¹‰æ›´æ¸…æ™°ï¼Œè€Œéç®€å•åœ°å¢åŠ å­—æ•°ã€‚

æœç»è¿‡åº¦å£è¯­åŒ–ï¼š
- ç»´æŒä¹¦é¢è¯­é£æ ¼ï¼šä¸¥ç¦ä½¿ç”¨"è‡³äºxxxå‘¢"ã€"è¿™ä¸ªå˜›"ç­‰å¸¦æœ‰æ˜æ˜¾å£è¯­æˆ–è¯­æ°”åŠ©è¯çš„è¡¨è¾¾æ–¹å¼ã€‚

æ ¸å¿ƒä¿®æ”¹æ‰‹æ³•ä¸è§„åˆ™

1. è§£é‡Šæ€§æ‰©å±•ï¼ˆVerbose Elaborationï¼‰

åŠ¨è¯çŸ­è¯­æ‰©å±•ï¼šå°†ç®€æ´çš„åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­ï¼Œæ›¿æ¢ä¸ºæ›´èƒ½ä½“ç°åŠ¨ä½œè¿‡ç¨‹çš„è¡¨è¾¾ã€‚
- "ç®¡ç†" -> "å¼€å±•...çš„ç®¡ç†å·¥ä½œ" æˆ– "è¿›è¡Œ...çš„ç®¡ç†"
- "äº¤äº’" -> "è¿›è¡Œæ•°æ®äº¤äº’" æˆ– "ä¸...å¼€å±•äº¤äº’"
- "é…ç½®" -> "è¿›è¡Œå‚æ•°é…ç½®" æˆ– "å¯¹...å®Œæˆé…ç½®"
- "å¤„ç†" -> "æ‰§è¡Œ...çš„å¤„ç†å·¥ä½œ" æˆ– "å¯¹...è¿›è¡Œå¤„ç†"
- "æ¢å¤" -> "æ‰§è¡Œæ¢å¤æ“ä½œ"
- "å®ç°" -> "å¾—ä»¥å®ç°" æˆ– "ç”¨ä»¥å®ç°"

å¢åŠ è¾…åŠ©è¯/ç»“æ„ï¼šåœ¨å¥å­ä¸­å®¡æ…åœ°æ·»åŠ è¯­æ³•ä¸Šå…è®¸ä½†éå¿…éœ€çš„è¯è¯­ï¼Œä½¿å¥å­æ›´é¥±æ»¡ã€‚
- é€‚å½“å¢åŠ  "äº†"ã€"çš„"ã€"åœ°"ã€"æ‰€"ã€"å¯ä»¥"ã€"è¯¥"ã€"ç›¸åº”" ç­‰ã€‚
- "æä¾›åŠŸèƒ½" -> "å…·å¤‡...çš„åŠŸèƒ½" æˆ– "èƒ½å¤Ÿæä¾›...åŠŸèƒ½"

2. ç³»ç»Ÿæ€§è¯æ±‡æ›¿æ¢ï¼ˆSystematic Phrasing Substitutionï¼‰

ç‰¹å®šåŠ¨è¯/ä»‹è¯/è¿è¯æ›¿æ¢ï¼š
- é‡‡ç”¨ / ä½¿ç”¨ -> è¿ç”¨ / é€‰ç”¨
- åŸºäº -> ä¾æ® / æ ¹æ®...æ¥
- åˆ©ç”¨ -> å€ŸåŠ© / è¿ç”¨
- é€šè¿‡ -> å€ŸåŠ© / ç»ç”±
- å’Œ / åŠ / ä¸ -> ä»¥åŠ
- å¹¶ -> å¹¶ä¸” / åŒæ—¶
- å…¶ -> è¯¥ / å…¶

ç‰¹å®šåè¯/å½¢å®¹è¯æ›¿æ¢ï¼š
- åŸå›  -> ç¼˜ç”±
- ç¬¦åˆ -> å¥‘åˆ
- é€‚åˆ -> é€‚å®œ
- ç‰¹ç‚¹ -> ç‰¹æ€§
- æå‡ / æé«˜ -> æå‡
- æå¤§(åœ°) -> åœ¨æå¤§ç¨‹åº¦ä¸Š
- ç«‹å³ -> å³åˆ»

3. æ‹¬å·å†…å®¹å¤„ç†ï¼ˆBracket Content Integrationï¼‰

è§£é‡Šæ€§æ‹¬å·ï¼šå¯¹äºåŸæ–‡ä¸­ç”¨äºè§£é‡Šã€ä¸¾ä¾‹æˆ–è¯´æ˜ç¼©å†™çš„æ‹¬å· (...) æˆ– ï¼ˆ...ï¼‰ï¼Œä¼˜å…ˆå°†å…¶ä¿¡æ¯è‡ªç„¶åœ°èå…¥å¥å­ã€‚
- èåˆæ–¹å¼ï¼šä½¿ç”¨ "å³"ã€"ä¹Ÿå°±æ˜¯"ã€"ä¾‹å¦‚" ç­‰å¼•å¯¼è¯è¿›è¡Œè¿æ¥ã€‚
- ç¤ºä¾‹ï¼šORMï¼ˆå¯¹è±¡å…³ç³»æ˜ å°„ï¼‰ -> å¯¹è±¡å…³ç³»æ˜ å°„ï¼ˆORMï¼‰æŠ€æœ¯
- ç¤ºä¾‹ï¼šåŠŸèƒ½ï¼ˆå¦‚ORMã€Adminï¼‰ -> å…·ä½“åŠŸèƒ½ï¼Œä¾‹å¦‚ORMä¸Admin

ä»£ç /æ ‡è¯†ç¬¦æ—æ‹¬å·ï¼šå¯¹äºç´§è·Ÿåœ¨ä»£ç ã€æ–‡ä»¶åã€ç±»åæ—çš„æ‹¬å·ï¼Œé€šå¸¸ç›´æ¥ç§»é™¤æ‹¬å·ï¼Œå°†å…¶ä½œä¸ºæ–‡æœ¬çš„è‡ªç„¶ç»„æˆéƒ¨åˆ†ã€‚
- ç¤ºä¾‹ï¼šè§†å›¾ (views.py) ä¸­ -> åœ¨è§†å›¾æ–‡ä»¶views.pyä¸­
- ç¤ºä¾‹ï¼šæƒé™ç±» (admin_panel.permissions) -> æƒé™ç±»admin_panel.permissions

4. å¥å¼å¾®è°ƒï¼ˆSentence Structure Refinementï¼‰

ä½¿ç”¨"æŠŠ"å­—å¥ï¼šåœ¨åˆé€‚çš„åœºæ™¯ä¸‹ï¼Œå¯é€‰ç”¨"æŠŠ"å­—å¥ä»¥ä¼˜åŒ–è¯­åºã€‚
- ç¤ºä¾‹ï¼š"ä¼šå°†å¯¹è±¡ç§»åŠ¨" -> "ä¼šæŠŠç›¸åº”çš„å¯¹è±¡ç§»åŠ¨"

æ¡ä»¶å¥å¼è½¬æ¢ï¼šå°†è¾ƒä¹¦é¢çš„æ¡ä»¶å¥å¼æ”¹ä¸ºä¸¥è°¨çš„è¡¨è¾¾ã€‚
- ç¤ºä¾‹ï¼š"è‹¥...ï¼Œåˆ™..." -> "å¦‚æœ...ï¼Œé‚£ä¹ˆ..." æˆ– "åœ¨...çš„æƒ…å†µä¸‹ï¼Œä¾¿ä¼š..."

å¢åŠ é€»è¾‘è¿æ¥è¯ï¼šåœ¨å¥é¦–æˆ–å¥ä¸­é€‚å½“æ·»åŠ "é¦–å…ˆ"ã€"å…¶æ¬¡"ã€"æ­¤å¤–"ã€"å› æ­¤"ã€"ç»¼ä¸Šæ‰€è¿°"ç­‰è¯è¯­ï¼Œä»¥å¢å¼ºé€»è¾‘çš„æ¸…æ™°åº¦ã€‚

è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™è¿›è¡Œæ”¹å†™ï¼Œç›´æ¥è¾“å‡ºä¿®æ”¹åçš„æ–‡æœ¬ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–é™AIå¼•æ“"""
        self._llm = None
    
    @property
    def llm(self):
        """å»¶è¿ŸåŠ è½½LLMå®¢æˆ·ç«¯"""
        if self._llm is None:
            self._llm = get_llm_client()
        return self._llm
    
    def process(self, content: str) -> DeAIResult:
        """
        æ‰§è¡Œé™AIå¤„ç†
        
        Args:
            content: åŸå§‹æ–‡æœ¬
            
        Returns:
            DeAIResult: é™AIç»“æœ
        """
        # 1. æ£€æµ‹AIç—•è¿¹
        ai_score_before = self.estimate_ai_score(content)
        
        # 2. å¤šç­–ç•¥äººæ€§åŒ–æ”¹å†™
        processed = self._humanize(content)
        
        # 3. å†æ¬¡æ£€æµ‹
        ai_score_after = self.estimate_ai_score(processed)
        
        # 4. è¯†åˆ«å˜åŒ–
        changes = self._identify_changes(content, processed)
        
        return DeAIResult(
            original=content,
            processed=processed,
            ai_score_before=ai_score_before,
            ai_score_after=ai_score_after,
            changes=changes,
            sentences_processed=len(split_sentences(content))
        )
    
    def estimate_ai_score(self, text: str) -> float:
        """
        ä¼°ç®—AIå†™ä½œæ¦‚ç‡
        
        åŸºäºå¤šç»´åº¦ç‰¹å¾æ£€æµ‹
        """
        if not text or len(text) < 30:
            return 0.0
        
        scores = []
        weights = []
        
        text_len_factor = len(text) / 1000
        
        # 1. æ£€æµ‹ç»“æ„åŒ–åºåˆ—ï¼ˆæƒé‡20%ï¼‰
        sequence_count = sum(
            text.count(m) for m in self.AI_MARKERS["sequence_markers"]
        )
        normalized_seq = sequence_count / max(1, text_len_factor)
        sequence_score = min(100, normalized_seq * 12)
        scores.append(sequence_score)
        weights.append(0.20)
        
        # 2. æ£€æµ‹å¡«å……çŸ­è¯­ï¼ˆæƒé‡25%ï¼‰
        filler_count = sum(
            text.count(p) for p in self.AI_MARKERS["filler_phrases"]
        )
        normalized_filler = filler_count / max(1, text_len_factor)
        filler_score = min(100, normalized_filler * 20)
        scores.append(filler_score)
        weights.append(0.25)
        
        # 3. æ£€æµ‹æ¨¡ç³Šè¡¨è¾¾ï¼ˆæƒé‡15%ï¼‰
        vague_count = sum(
            text.count(e) for e in self.AI_MARKERS["vague_expressions"]
        )
        normalized_vague = vague_count / max(1, text_len_factor)
        vague_score = min(100, normalized_vague * 15)
        scores.append(vague_score)
        weights.append(0.15)
        
        # 4. æ£€æµ‹å¥å­é•¿åº¦å‡åŒ€åº¦ï¼ˆæƒé‡20%ï¼‰
        sentences = split_sentences(text)
        if len(sentences) >= 3:
            lengths = [len(s) for s in sentences if len(s) > 5]
            if lengths:
                avg_len = sum(lengths) / len(lengths)
                variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
                std_dev = variance ** 0.5
                
                if std_dev < 10:
                    uniformity_score = 90
                elif std_dev < 20:
                    uniformity_score = 60
                elif std_dev < 30:
                    uniformity_score = 30
                else:
                    uniformity_score = 10
                scores.append(uniformity_score)
                weights.append(0.20)
        
        # 5. è¿‡åº¦æ­£å¼è¡¨è¾¾ï¼ˆæƒé‡10%ï¼‰
        formal_count = sum(
            text.count(f) for f in self.AI_MARKERS["overly_formal"]
        )
        normalized_formal = formal_count / max(1, text_len_factor)
        formal_score = min(100, normalized_formal * 18)
        scores.append(formal_score)
        weights.append(0.10)
        
        # 6. è¿æ¥è¯æ»¥ç”¨ï¼ˆæƒé‡10%ï¼‰
        connector_count = sum(
            text.count(c) for c in self.AI_MARKERS["connector_abuse"]
        )
        normalized_connector = connector_count / max(1, text_len_factor)
        connector_score = min(100, normalized_connector * 8)
        scores.append(connector_score)
        weights.append(0.10)
        
        # è®¡ç®—åŠ æƒå¹³å‡
        if scores and weights:
            total_weight = sum(weights)
            weighted_sum = sum(s * w for s, w in zip(scores, weights))
            return weighted_sum / total_weight
        
        return 50.0
    
    def detect_ai_features(self, text: str) -> AIDetectionResult:
        """è¯¦ç»†æ£€æµ‹AIç‰¹å¾"""
        dimensions = {}
        ai_markers = []
        suggestions = []
        
        for category, markers in self.AI_MARKERS.items():
            found = [m for m in markers if m in text]
            count = len(found)
            
            if category == "sequence_markers":
                dimensions["ç»“æ„è§„æ•´åº¦"] = min(10, count * 2)
                if found:
                    ai_markers.extend([f"åºåˆ—è¯: {m}" for m in found[:3]])
                    suggestions.append("å‡å°‘ä½¿ç”¨'é¦–å…ˆã€å…¶æ¬¡ã€æœ€å'ç­‰åºåˆ—è¯ï¼Œæ”¹ç”¨æ›´è‡ªç„¶çš„è¿‡æ¸¡")
                    
            elif category == "filler_phrases":
                dimensions["å¡«å……çŸ­è¯­"] = min(10, count * 3)
                if found:
                    ai_markers.extend([f"å¡«å……è¯­: {m}" for m in found[:3]])
                    suggestions.append("åˆ é™¤'å€¼å¾—æ³¨æ„çš„æ˜¯'ç­‰å¡«å……æ€§çŸ­è¯­")
                    
            elif category == "vague_expressions":
                dimensions["æ¨¡ç³Šè¡¨è¾¾"] = min(10, count * 2)
                if found:
                    ai_markers.extend([f"æ¨¡ç³Šè¯­: {m}" for m in found[:3]])
                    suggestions.append("ä½¿ç”¨æ›´å…·ä½“çš„è¡¨è¿°æ›¿ä»£æ¨¡ç³Šè¡¨è¾¾")
                    
            elif category == "overly_formal":
                dimensions["è¿‡åº¦æ­£å¼"] = min(10, count * 3)
                if found:
                    ai_markers.extend([f"æ­£å¼è¯­: {m}" for m in found[:3]])
                    suggestions.append("é€‚å½“é™ä½è¯­è¨€çš„æ­£å¼ç¨‹åº¦")
        
        # æ£€æµ‹å¥å­å‡åŒ€åº¦
        sentences = split_sentences(text)
        if len(sentences) >= 3:
            lengths = [len(s) for s in sentences]
            avg_len = sum(lengths) / len(lengths)
            variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
            uniformity = max(0, 10 - variance / 50)
            dimensions["å¥å¼å‡åŒ€åº¦"] = uniformity
            if uniformity > 7:
                suggestions.append("å˜åŒ–å¥å­é•¿åº¦ï¼Œæ‰“ç ´å‡åŒ€èŠ‚å¥")
        
        overall = sum(dimensions.values()) / len(dimensions) * 10 if dimensions else 50
        
        return AIDetectionResult(
            overall_score=overall,
            dimensions=dimensions,
            ai_markers=ai_markers,
            suggestions=suggestions
        )
    
    def _humanize(self, content: str) -> str:
        """
        äººæ€§åŒ–æ”¹å†™ - å¤šç­–ç•¥ç»„åˆ
        
        1. å…ˆè¿›è¡Œè§„åˆ™æ›¿æ¢ï¼ˆå¿«é€Ÿï¼‰
        2. å†ç”¨LLMç²¾ä¿®ï¼ˆæ·±åº¦ï¼‰
        """
        # Step 1: è§„åˆ™é¢„å¤„ç†
        pre_processed = self._rule_based_humanize(content)
        
        # Step 2: LLM ç²¾ä¿®
        try:
            processed = self._llm_humanize(pre_processed)
            return processed
        except Exception:
            # å¦‚æœLLMè°ƒç”¨å¤±è´¥ï¼Œè¿”å›è§„åˆ™å¤„ç†ç»“æœ
            return pre_processed
    
    def _rule_based_humanize(self, content: str) -> str:
        """
        åŸºäºè§„åˆ™çš„äººæ€§åŒ–æ”¹å†™
        """
        result = content
        
        # 1. åˆ é™¤/æ›¿æ¢å¡«å……çŸ­è¯­
        for old, new in self.FILLER_REPLACEMENTS.items():
            result = result.replace(old, new)
        
        # 2. è¯æ±‡æ›¿æ¢
        for word, replacements in self.WORD_SUBSTITUTIONS.items():
            if word in result:
                replacement = random.choice(replacements)
                # åªæ›¿æ¢éƒ¨åˆ†ï¼Œä¿æŒè‡ªç„¶
                count = result.count(word)
                replace_count = max(1, count // 2)
                for _ in range(replace_count):
                    result = result.replace(word, replacement, 1)
        
        # 3. åŠ¨è¯æ‰©å±• (é€‰æ‹©æ€§)
        for verb, expansions in self.VERB_EXPANSIONS.items():
            if verb in result and random.random() < 0.3:
                expansion = random.choice(expansions)
                result = result.replace(verb, expansion, 1)
        
        # 4. å¥å¼è½¬æ¢
        for pattern, replacement in self.SENTENCE_TRANSFORMS:
            if random.random() < 0.5:
                result = re.sub(pattern, replacement, result)
        
        # 5. æ‰“ç ´è§„æ•´çš„åºåˆ—ç»“æ„
        sequence_replacements = {
            "é¦–å…ˆï¼Œ": "",
            "å…¶æ¬¡ï¼Œ": "åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ",
            "å†æ¬¡ï¼Œ": "åŒæ ·å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œ",
            "æœ€åï¼Œ": "æ›´ä¸ºé‡è¦çš„æ˜¯ï¼Œ",
            "ä¸€æ–¹é¢ï¼Œ": "ä»ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œ",
            "å¦ä¸€æ–¹é¢ï¼Œ": "ä»å¦ä¸€ä¸ªç»´åº¦æ¥çœ‹ï¼Œ",
        }
        for old, new in sequence_replacements.items():
            result = result.replace(old, new, 1)
        
        return result
    
    def _llm_humanize(self, content: str) -> str:
        """
        ä½¿ç”¨LLMè¿›è¡Œäººæ€§åŒ–æ”¹å†™
        """
        # åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬
        if len(content) > 2000:
            paragraphs = content.split("\n\n")
            processed_paragraphs = []
            for para in paragraphs:
                if len(para.strip()) < 50:
                    processed_paragraphs.append(para)
                else:
                    processed_paragraphs.append(self._llm_humanize_single(para))
            return "\n\n".join(processed_paragraphs)
        else:
            return self._llm_humanize_single(content)
    
    def _llm_humanize_single(self, content: str) -> str:
        """
        LLMæ”¹å†™å•æ®µæ–‡æœ¬
        """
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬è¿›è¡Œæ”¹å†™ï¼Œæ¶ˆé™¤AIå†™ä½œç—•è¿¹ï¼Œä½¿å…¶æ›´åƒäººç±»å­¦è€…çš„è¡¨è¾¾ã€‚

## æ”¹å†™è¦æ±‚
1. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¸“ä¸šæœ¯è¯­
2. æ¶ˆé™¤è§„æ•´çš„åºåˆ—ç»“æ„ï¼ˆé¦–å…ˆã€å…¶æ¬¡ã€æœ€åï¼‰
3. åˆ é™¤å¡«å……æ€§çŸ­è¯­ï¼ˆå€¼å¾—æ³¨æ„çš„æ˜¯ã€ç»¼ä¸Šæ‰€è¿°ç­‰ï¼‰
4. å˜åŒ–å¥å­é•¿åº¦ï¼Œé¿å…è¿‡äºå‡åŒ€
5. ä½¿ç”¨æ›´è‡ªç„¶çš„è¿‡æ¸¡å’Œè¿æ¥
6. ä¿æŒå­¦æœ¯è§„èŒƒæ€§å’Œä¸“ä¸šæ€§
7. è¾“å‡ºé•¿åº¦ä¸åŸæ–‡å¤§è‡´ç›¸ç­‰

## åŸæ–‡
{content}

## è¦æ±‚
ç›´æ¥è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šè¯´æ˜ã€‚"""

        try:
            processed = self.llm.invoke(
                prompt,
                system_prompt=self.SYSTEM_PROMPT,
                temperature=0.75  # ç¨é«˜çš„æ¸©åº¦å¢åŠ å˜åŒ–æ€§
            )
            return processed.strip()
        except Exception:
            return content
    
    def _identify_changes(self, original: str, processed: str) -> List[str]:
        """è¯†åˆ«ä¸»è¦å˜åŒ–"""
        changes = []
        
        # æ£€æµ‹åºåˆ—è¯å˜åŒ–
        orig_seq = sum(original.count(m) for m in self.AI_MARKERS["sequence_markers"])
        proc_seq = sum(processed.count(m) for m in self.AI_MARKERS["sequence_markers"])
        if proc_seq < orig_seq:
            changes.append(f"å‡å°‘äº†{orig_seq - proc_seq}å¤„åºåˆ—æ€§è¯æ±‡")
        
        # æ£€æµ‹å¡«å……çŸ­è¯­å˜åŒ–
        orig_fill = sum(original.count(p) for p in self.AI_MARKERS["filler_phrases"])
        proc_fill = sum(processed.count(p) for p in self.AI_MARKERS["filler_phrases"])
        if proc_fill < orig_fill:
            changes.append(f"åˆ é™¤äº†{orig_fill - proc_fill}å¤„å¡«å……æ€§çŸ­è¯­")
        
        # è¯æ±‡æ›¿æ¢æ£€æµ‹
        replaced_count = 0
        for word in self.WORD_SUBSTITUTIONS.keys():
            if word in original and word not in processed:
                replaced_count += 1
        if replaced_count > 0:
            changes.append(f"è¿›è¡Œäº†{replaced_count}å¤„è¯æ±‡ä¼˜åŒ–")
        
        # å¥å¼å˜åŒ–æ£€æµ‹
        orig_sentences = len(split_sentences(original))
        proc_sentences = len(split_sentences(processed))
        if abs(proc_sentences - orig_sentences) >= 2:
            changes.append("è°ƒæ•´äº†å¥å­ç»“æ„")
        
        # é•¿åº¦å˜åŒ–
        len_ratio = len(processed) / len(original) if len(original) > 0 else 1
        if len_ratio < 0.95:
            changes.append("ç²¾ç®€äº†å†—ä½™è¡¨è¾¾")
        elif len_ratio > 1.05:
            changes.append("å¢å¼ºäº†è§£é‡Šæ€§è¡¨è¾¾")
        
        if not changes:
            changes.append("è°ƒæ•´äº†è¡¨è¾¾æ–¹å¼ï¼Œå¢å¼ºè‡ªç„¶åº¦")
        
        return changes[:5]
    
    def get_report(self, result: DeAIResult) -> str:
        """ç”Ÿæˆé™AIæŠ¥å‘Š"""
        lines = []
        lines.append("## ğŸ¤– é™AIå¤„ç†æŠ¥å‘Š\n")
        
        # AIæ¦‚ç‡å˜åŒ–
        reduction = result.ai_score_before - result.ai_score_after
        reduction_pct = (reduction / result.ai_score_before * 100) if result.ai_score_before > 0 else 0
        
        lines.append("### AIæ¦‚ç‡å˜åŒ–")
        lines.append(f"- å¤„ç†å‰AIæ¦‚ç‡ï¼š{result.ai_score_before:.1f}%")
        lines.append(f"- å¤„ç†åAIæ¦‚ç‡ï¼š{result.ai_score_after:.1f}%")
        lines.append(f"- **é™ä½å¹…åº¦ï¼š{reduction:.1f}% ({reduction_pct:.0f}%)**")
        lines.append(f"- å¤„ç†å¥å­æ•°ï¼š{result.sentences_processed}")
        lines.append("")
        
        # æ•ˆæœè¯„ä¼°
        if result.ai_score_after < 30:
            lines.append("### æ•ˆæœè¯„ä¼°")
            lines.append("âœ… **ä¼˜ç§€** - AIç—•è¿¹å·²åŸºæœ¬æ¶ˆé™¤ï¼Œæ–‡æœ¬è‡ªç„¶åº¦é«˜")
        elif result.ai_score_after < 50:
            lines.append("### æ•ˆæœè¯„ä¼°")
            lines.append("âš ï¸ **è‰¯å¥½** - AIç—•è¿¹æ˜¾è‘—å‡å°‘ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            lines.append("### æ•ˆæœè¯„ä¼°")
            lines.append("âŒ **éœ€æ”¹è¿›** - ä»æœ‰æ˜æ˜¾AIç—•è¿¹ï¼Œå»ºè®®æ‰‹åŠ¨è°ƒæ•´")
        lines.append("")
        
        # ä¸»è¦å˜åŒ–
        lines.append("### ä¸»è¦å˜åŒ–")
        for change in result.changes:
            lines.append(f"- {change}")
        
        return "\n".join(lines)
