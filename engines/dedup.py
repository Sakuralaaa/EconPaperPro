# -*- coding: utf-8 -*-
"""
é™é‡å¼•æ“æ¨¡å— (ä¼˜åŒ–ç‰ˆ)
å­¦æœ¯çº§æ–‡æœ¬æ”¹å†™ï¼Œé™ä½é‡å¤ç‡
- å¤šç­–ç•¥æ”¹å†™
- åˆ†å¥ç²¾ç»†å¤„ç†
- è¯­ä¹‰ä¿çœŸéªŒè¯
"""

from typing import List, Optional, Dict, Tuple
from dataclasses import dataclass, field
import re
import random
from core.llm import get_llm_client
from core.prompts import PromptTemplates

# é…ç½®å¸¸é‡
MAX_CHANGES_TO_REPORT = 5
SENTENCE_MAX_LENGTH = 300  # å•å¥æœ€å¤§é•¿åº¦
BATCH_SIZE = 3  # æ‰¹é‡å¤„ç†å¥å­æ•°


@dataclass
class DedupResult:
    """é™é‡ç»“æœ"""
    original: str
    processed: str
    changes: List[str]
    similarity_before: float
    similarity_after: float
    preserved_terms: List[str] = field(default_factory=list)
    sentences_processed: int = 0


class DedupEngine:
    """
    é™é‡å¼•æ“ (ä¼˜åŒ–ç‰ˆ)
    
    æ”¹è¿›ç‚¹ï¼š
    1. åˆ†å¥ç²¾ç»†å¤„ç†ï¼Œé¿å…é•¿æ–‡æœ¬ä¸€æ¬¡æ€§æ”¹å†™å¯¼è‡´è¯­ä¹‰åç§»
    2. å¤šè½®æ”¹å†™ç­–ç•¥ï¼Œå¼ºåº¦å¯æ§
    3. æœ¯è¯­ä¿æŠ¤æœºåˆ¶å¢å¼º
    4. è¯­ä¹‰ç›¸ä¼¼åº¦éªŒè¯
    """
    
    # é»˜è®¤ä¿ç•™çš„å­¦æœ¯æœ¯è¯­ (æ‰©å±•ç‰ˆ)
    DEFAULT_PRESERVE_TERMS = [
        # è®¡é‡æ–¹æ³•
        "åŒé‡å·®åˆ†", "DID", "difference-in-differences", "å·®åˆ†",
        "å€¾å‘å¾—åˆ†åŒ¹é…", "PSM", "propensity score matching",
        "å·¥å…·å˜é‡", "IV", "instrumental variable", "2SLS", "ä¸¤é˜¶æ®µæœ€å°äºŒä¹˜",
        "æ–­ç‚¹å›å½’", "RDD", "regression discontinuity", "æ–­ç‚¹è®¾è®¡",
        "å›ºå®šæ•ˆåº”", "fixed effects", "FE", "ä¸ªä½“å›ºå®šæ•ˆåº”", "æ—¶é—´å›ºå®šæ•ˆåº”",
        "éšæœºæ•ˆåº”", "random effects", "RE",
        "é¢æ¿æ•°æ®", "panel data", "å¹³è¡¡é¢æ¿", "éå¹³è¡¡é¢æ¿",
        "å¹¿ä¹‰çŸ©ä¼°è®¡", "GMM", "ç³»ç»ŸGMM", "å·®åˆ†GMM",
        "ä¸­ä»‹æ•ˆåº”", "mediating effect", "ä¸­ä»‹å˜é‡",
        "è°ƒèŠ‚æ•ˆåº”", "moderating effect", "è°ƒèŠ‚å˜é‡",
        "å¼‚è´¨æ€§åˆ†æ", "åˆ†æ ·æœ¬å›å½’",
        "åˆæˆæ§åˆ¶æ³•", "SCM", "synthetic control",
        "äº‹ä»¶ç ”ç©¶æ³•", "event study",
        # ç»Ÿè®¡æœ¯è¯­
        "æ˜¾è‘—æ€§", "significance", "æ˜¾è‘—",
        "ç¨³å¥æ€§", "robustness", "ç¨³å¥æ€§æ£€éªŒ",
        "å†…ç”Ÿæ€§", "endogeneity", "å†…ç”Ÿæ€§é—®é¢˜",
        "å¼‚æ–¹å·®", "heteroskedasticity", "å¼‚æ–¹å·®æ£€éªŒ",
        "è‡ªç›¸å…³", "autocorrelation", "åºåˆ—ç›¸å…³",
        "å¤šé‡å…±çº¿æ€§", "multicollinearity", "VIF",
        "tç»Ÿè®¡é‡", "tå€¼", "t-statistic",
        "Fç»Ÿè®¡é‡", "Få€¼", "F-test",
        "Ræ–¹", "RÂ²", "R-squared", "è°ƒæ•´Ræ–¹",
        "æ ‡å‡†è¯¯", "standard error", "èšç±»æ ‡å‡†è¯¯",
        "ç½®ä¿¡åŒºé—´", "confidence interval",
        "på€¼", "p-value", "æ˜¾è‘—æ€§æ°´å¹³",
        "Bootstrap", "è‡ªåŠ©æ³•",
        # ç»æµå­¦æœ¯è¯­
        "è¾¹é™…æ•ˆåº”", "marginal effect",
        "å¼¹æ€§", "elasticity", "ä»·æ ¼å¼¹æ€§",
        "å¤–éƒ¨æ€§", "externality", "æ­£å¤–éƒ¨æ€§", "è´Ÿå¤–éƒ¨æ€§",
        "ä¿¡æ¯ä¸å¯¹ç§°", "information asymmetry",
        "å§”æ‰˜ä»£ç†", "principal-agent", "ä»£ç†é—®é¢˜",
        "é“å¾·é£é™©", "moral hazard",
        "é€†å‘é€‰æ‹©", "adverse selection",
        "äº¤æ˜“æˆæœ¬", "transaction cost",
        "è§„æ¨¡ç»æµ", "economies of scale",
        "èŒƒå›´ç»æµ", "economies of scope",
        # é‡‘èæœ¯è¯­
        "èµ„äº§å®šä»·", "asset pricing", "CAPM",
        "å¸‚åœºæœ‰æ•ˆæ€§", "market efficiency",
        "ä¿¡æ¯æ•ˆç‡", "information efficiency",
        "èèµ„çº¦æŸ", "financing constraints",
        "ä»£ç†æˆæœ¬", "agency cost",
        # æ•°æ®å’Œå˜é‡
        "è¢«è§£é‡Šå˜é‡", "è§£é‡Šå˜é‡", "æ§åˆ¶å˜é‡",
        "è™šæ‹Ÿå˜é‡", "äº¤äº’é¡¹", "æ»åé¡¹",
    ]
    
    # åŒä¹‰è¯æ›¿æ¢è¯å…¸ (è§„åˆ™æ›¿æ¢åŠ é€Ÿ)
    SYNONYM_DICT = {
        "ç ”ç©¶": ["æ¢è®¨", "åˆ†æ", "è€ƒå¯Ÿ", "æ¢ç©¶", "å®¡è§†"],
        "è¡¨æ˜": ["æ˜¾ç¤º", "è¯´æ˜", "æ­ç¤º", "åæ˜ ", "å°è¯"],
        "å‘ç°": ["è§‚å¯Ÿåˆ°", "è¯†åˆ«å‡º", "æ³¨æ„åˆ°", "å¯Ÿè§‰"],
        "å½±å“": ["ä½œç”¨", "æ•ˆåº”", "å†²å‡»", "æ³¢åŠ"],
        "æå‡": ["æé«˜", "å¢å¼º", "ä¿ƒè¿›", "æ”¹å–„", "ä¼˜åŒ–"],
        "é™ä½": ["å‡å°‘", "å‰Šå¼±", "æŠ‘åˆ¶", "å‰Šå‡"],
        "é‡è¦": ["å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "é¦–è¦", "æ˜¾è‘—"],
        "å› æ­¤": ["æ•…æ­¤", "ç”±æ­¤", "æ®æ­¤", "ä»è€Œ"],
        "ç„¶è€Œ": ["ä½†æ˜¯", "ä¸è¿‡", "å¯æ˜¯", "ç„¶åˆ™"],
        "åŒæ—¶": ["ä¸æ­¤åŒæ—¶", "æ­¤å¤–", "å¦å¤–", "å¹¶ä¸”"],
        "é€šè¿‡": ["å€ŸåŠ©", "ä¾æ‰˜", "å‡­å€Ÿ", "ç»ç”±"],
        "é‡‡ç”¨": ["è¿ç”¨", "ä½¿ç”¨", "åº”ç”¨", "åˆ©ç”¨"],
        "è¿›è¡Œ": ["å¼€å±•", "å®æ–½", "æ‰§è¡Œ", "æ¨è¿›"],
        "å…·æœ‰": ["æ‹¥æœ‰", "å­˜åœ¨", "å‘ˆç°", "è¡¨ç°å‡º"],
        "æ˜¾è‘—": ["æ˜æ˜¾", "çªå‡º", "æ˜¾è‘—æ€§", "notable"],
        "åŸºäº": ["ä¾æ®", "æ ¹æ®", "ç«‹è¶³äº", "ä»¥...ä¸ºåŸºç¡€"],
        "é’ˆå¯¹": ["é¢å‘", "å°±...è€Œè¨€", "å…³äº"],
        "å¯¼è‡´": ["å¼•èµ·", "å¼•å‘", "é€ æˆ", "å¸¦æ¥"],
        "å¢åŠ ": ["å¢é•¿", "ä¸Šå‡", "æ‰©å¤§", "æå‡"],
        "è¯æ˜": ["éªŒè¯", "è¯å®", "è¡¨æ˜", "ä½è¯"],
        "è®¤ä¸º": ["æŒ‡å‡º", "æå‡º", "ä¸»å¼ ", "å¼ºè°ƒ"],
        "å¯ä»¥": ["èƒ½å¤Ÿ", "å¯", "å¾—ä»¥"],
        "éœ€è¦": ["é¡»è¦", "æœ‰å¿…è¦", "äºŸéœ€"],
        "é—®é¢˜": ["è®®é¢˜", "éš¾é¢˜", "å›°å¢ƒ", "æŒ‘æˆ˜"],
        "æ–¹æ³•": ["é€”å¾„", "æ–¹å¼", "æ‰‹æ®µ", "ç­–ç•¥"],
        "ç»“æœ": ["å‘ç°", "ç»“è®º", "æˆæœ", "äº§å‡º"],
        "æƒ…å†µ": ["çŠ¶å†µ", "æ€åŠ¿", "æƒ…å½¢", "å¢ƒå†µ"],
        "æ°´å¹³": ["ç¨‹åº¦", "å±‚æ¬¡", "çº§åˆ«"],
        "èƒ½åŠ›": ["å®åŠ›", "ç´ è´¨", "æ½œåŠ›", "æœ¬é¢†"],
        "ä½œç”¨": ["åŠŸèƒ½", "æ•ˆæœ", "å½±å“", "åŠŸæ•ˆ"],
        "ç‰¹ç‚¹": ["ç‰¹å¾", "å±æ€§", "ç‰¹æ€§", "ç‰¹è´¨"],
        "å…³ç³»": ["è”ç³»", "å…³è”", "ç›¸å…³æ€§", "çº½å¸¦"],
        "åˆ†æ": ["å‰–æ", "è§£æ", "ç ”åˆ¤", "æ¢æ"],
        "ç›®å‰": ["å½“å‰", "ç°é˜¶æ®µ", "å¦‚ä»Š", "å½“ä¸‹"],
        "å·²ç»": ["ä¸šå·²", "å·²ç„¶", "æ—¢å·²"],
        "å¯èƒ½": ["æˆ–è®¸", "ä¹Ÿè®¸", "å¤§æ¦‚", "å…´è®¸"],
        "å®ç°": ["è¾¾æˆ", "å®Œæˆ", "å–å¾—", "è¾¾åˆ°"],
        "ä¿ƒè¿›": ["æ¨åŠ¨", "åŠ©åŠ›", "é©±åŠ¨", "å¸¦åŠ¨"],
        "æ”¯æŒ": ["æ”¯æ’‘", "ä½è¯", "å°è¯", "éªŒè¯"],
        "äº§ç”Ÿ": ["å½¢æˆ", "å‡ºç°", "å¼•å‘", "å‚¬ç”Ÿ"],
        "å˜åŒ–": ["å˜åŠ¨", "æ”¹å˜", "è½¬å˜", "æ¼”å˜"],
        "åˆ©ç”¨": ["è¿ç”¨", "å€ŸåŠ©", "ä¾æ‰˜", "å‡­å€Ÿ"],
    }
    
    # å¥å¼è½¬æ¢æ¨¡å¼
    SENTENCE_PATTERNS = [
        # (åŸå§‹æ¨¡å¼, æ›¿æ¢æ¨¡å¼)
        (r"^(.+?)å¯¹(.+?)äº§ç”Ÿäº†(.+?)å½±å“", r"\2å—åˆ°\1çš„\3å½±å“"),
        (r"^(.+?)ä¿ƒè¿›äº†(.+?)çš„å‘å±•", r"\2çš„å‘å±•å¾—åˆ°äº†\1çš„ä¿ƒè¿›"),
        (r"^ç ”ç©¶è¡¨æ˜[,ï¼Œ](.+)", r"å®è¯ç»“æœæ˜¾ç¤ºï¼Œ\1"),
        (r"^æœ¬æ–‡å‘ç°[,ï¼Œ](.+)", r"åˆ†æå‘ç°ï¼Œ\1"),
        (r"^ç»“æœæ˜¾ç¤º[,ï¼Œ](.+)", r"å®è¯åˆ†æè¡¨æ˜ï¼Œ\1"),
        (r"^(.+?)æ˜¾è‘—æå‡äº†(.+)", r"\2åœ¨\1ä½œç”¨ä¸‹æ˜¾è‘—æå‡"),
        (r"^éšç€(.+?)çš„(.+?)[,ï¼Œ](.+)", r"ä¼´éš\1çš„\2ï¼Œ\3"),
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–é™é‡å¼•æ“"""
        self._llm = None
    
    @property
    def llm(self):
        """å»¶è¿ŸåŠ è½½LLMå®¢æˆ·ç«¯"""
        if self._llm is None:
            self._llm = get_llm_client()
        return self._llm
    
    def process(
        self,
        content: str,
        strength: int = 3,
        preserve_terms: Optional[List[str]] = None
    ) -> DedupResult:
        """
        æ‰§è¡Œé™é‡å¤„ç†
        
        Args:
            content: åŸå§‹æ–‡æœ¬
            strength: é™é‡å¼ºåº¦ (1-5)
            preserve_terms: éœ€è¦ä¿ç•™çš„ä¸“ä¸šæœ¯è¯­
            
        Returns:
            DedupResult: é™é‡ç»“æœ
        """
        strength = max(1, min(5, strength))
        
        # åˆå¹¶ä¿ç•™æœ¯è¯­
        all_terms = self.DEFAULT_PRESERVE_TERMS.copy()
        if preserve_terms:
            all_terms.extend(preserve_terms)
        
        # è¯†åˆ«æ–‡æœ¬ä¸­å‡ºç°çš„ä¿ç•™æœ¯è¯­
        found_terms = [t for t in all_terms if t in content]
        
        # æ ¹æ®å¼ºåº¦é€‰æ‹©å¤„ç†ç­–ç•¥
        if strength <= 2:
            # è½»åº¦ï¼šè§„åˆ™æ›¿æ¢ä¸ºä¸»
            processed = self._rule_based_rewrite(content, strength, found_terms)
        elif strength <= 4:
            # ä¸­åº¦ï¼šè§„åˆ™ + LLM æ··åˆ
            processed = self._hybrid_rewrite(content, strength, found_terms)
        else:
            # æ·±åº¦ï¼šLLM åˆ†å¥ç²¾ç»†æ”¹å†™
            processed = self._deep_rewrite(content, found_terms)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity_before = 1.0
        similarity_after = self._calculate_similarity(content, processed)
        
        # è¯†åˆ«å˜åŒ–
        changes = self._identify_changes(content, processed)
        
        return DedupResult(
            original=content,
            processed=processed,
            changes=changes,
            similarity_before=similarity_before,
            similarity_after=similarity_after,
            preserved_terms=found_terms,
            sentences_processed=len(self._split_sentences(content))
        )
    
    def _rule_based_rewrite(
        self,
        content: str,
        strength: int,
        preserve_terms: List[str]
    ) -> str:
        """
        åŸºäºè§„åˆ™çš„è½»åº¦æ”¹å†™
        """
        result = content
        
        # 1. åŒä¹‰è¯æ›¿æ¢ (ä¿æŠ¤æœ¯è¯­)
        for word, synonyms in self.SYNONYM_DICT.items():
            if word in preserve_terms:
                continue
            # éšæœºé€‰æ‹©æ›¿æ¢æ¦‚ç‡
            if random.random() < 0.3 * strength:
                replacement = random.choice(synonyms)
                result = result.replace(word, replacement, 1)
        
        # 2. å¥å¼è½¬æ¢
        if strength >= 2:
            for pattern, replacement in self.SENTENCE_PATTERNS:
                if random.random() < 0.4:
                    result = re.sub(pattern, replacement, result)
        
        return result
    
    def _hybrid_rewrite(
        self,
        content: str,
        strength: int,
        preserve_terms: List[str]
    ) -> str:
        """
        æ··åˆæ”¹å†™ï¼šè§„åˆ™é¢„å¤„ç† + LLM ç²¾ä¿®
        """
        # å…ˆè¿›è¡Œè§„åˆ™æ”¹å†™
        pre_processed = self._rule_based_rewrite(content, strength - 1, preserve_terms)
        
        # å¯¹é•¿æ–‡æœ¬åˆ†æ®µå¤„ç†
        paragraphs = content.split("\n\n")
        if len(paragraphs) > 1:
            processed_paragraphs = []
            for para in paragraphs:
                if len(para.strip()) < 50:
                    processed_paragraphs.append(para)
                else:
                    processed_paragraphs.append(
                        self._llm_rewrite_single(para, strength, preserve_terms)
                    )
            return "\n\n".join(processed_paragraphs)
        else:
            return self._llm_rewrite_single(pre_processed, strength, preserve_terms)
    
    def _deep_rewrite(
        self,
        content: str,
        preserve_terms: List[str]
    ) -> str:
        """
        æ·±åº¦æ”¹å†™ï¼šåˆ†å¥ç²¾ç»†å¤„ç†
        """
        sentences = self._split_sentences(content)
        processed_sentences = []
        
        # æ‰¹é‡å¤„ç†å¥å­
        batch = []
        for i, sentence in enumerate(sentences):
            if len(sentence.strip()) < 10:
                # çŸ­å¥ç›´æ¥ä¿ç•™
                if batch:
                    processed_sentences.extend(self._batch_rewrite(batch, preserve_terms))
                    batch = []
                processed_sentences.append(sentence)
            else:
                batch.append(sentence)
                if len(batch) >= BATCH_SIZE:
                    processed_sentences.extend(self._batch_rewrite(batch, preserve_terms))
                    batch = []
        
        # å¤„ç†å‰©ä½™
        if batch:
            processed_sentences.extend(self._batch_rewrite(batch, preserve_terms))
        
        return "".join(processed_sentences)
    
    def _batch_rewrite(
        self,
        sentences: List[str],
        preserve_terms: List[str]
    ) -> List[str]:
        """
        æ‰¹é‡æ”¹å†™å¥å­
        """
        combined = " ".join(sentences)
        rewritten = self._llm_rewrite_single(combined, 5, preserve_terms)
        
        # å°è¯•æ‹†åˆ†å›å¥å­
        rewritten_sentences = self._split_sentences(rewritten)
        
        # å¦‚æœæ‹†åˆ†åæ•°é‡æ¥è¿‘ï¼Œé€å¥è¿”å›ï¼›å¦åˆ™æ•´ä½“è¿”å›
        if abs(len(rewritten_sentences) - len(sentences)) <= 1:
            return rewritten_sentences
        else:
            return [rewritten]
    
    def _llm_rewrite_single(
        self,
        content: str,
        strength: int,
        preserve_terms: List[str]
    ) -> str:
        """
        LLM æ”¹å†™å•æ®µæ–‡æœ¬
        """
        # ä½¿ç”¨ä¼˜åŒ–çš„ prompt
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å­¦æœ¯æ–‡æœ¬è¿›è¡Œä¸“ä¸šæ”¹å†™ï¼Œé™ä½ä¸åŸæ–‡çš„ç›¸ä¼¼åº¦ã€‚

## æ”¹å†™è¦æ±‚
1. **è¯­ä¹‰ä¿çœŸ**ï¼šç¡®ä¿æ”¹å†™åæ„æ€å®Œå…¨ç›¸åŒ
2. **å­¦æœ¯è§„èŒƒ**ï¼šä¿æŒä¸¥è°¨çš„å­¦æœ¯è¡¨è¾¾é£æ ¼
3. **é™é‡å¼ºåº¦**ï¼š{strength}/5 ï¼ˆ1æœ€è½»ï¼Œ5æœ€æ·±ï¼‰
4. **å¿…é¡»ä¿ç•™çš„æœ¯è¯­**ï¼š{', '.join(preserve_terms[:15]) if preserve_terms else 'æ— '}

## æ”¹å†™æŠ€å·§
- åŒä¹‰è¯æ›¿æ¢ï¼ˆéæœ¯è¯­éƒ¨åˆ†ï¼‰
- ä¸»è¢«åŠ¨è¯­æ€è½¬æ¢
- å¥å­ç»“æ„è°ƒæ•´
- è¡¨è¾¾æ–¹å¼é‡æ„
- é€‚å½“æ‹†åˆ†æˆ–åˆå¹¶å¥å­

## åŸæ–‡
{content}

## è¦æ±‚
ç›´æ¥è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"""

        try:
            processed = self.llm.invoke(
                prompt,
                system_prompt="ä½ æ˜¯å­¦æœ¯å†™ä½œä¸“å®¶ï¼Œæ“…é•¿åœ¨ä¿æŒå­¦æœ¯è§„èŒƒå’Œè¯­ä¹‰çš„å‰æä¸‹æ”¹å†™æ–‡æœ¬ï¼Œé™ä½æ–‡æœ¬ç›¸ä¼¼åº¦ã€‚",
                temperature=0.7 + (strength * 0.05)  # å¼ºåº¦è¶Šé«˜æ¸©åº¦è¶Šé«˜
            )
            return processed.strip()
        except Exception as e:
            # å¤±è´¥æ—¶è¿”å›è§„åˆ™æ”¹å†™ç»“æœ
            return self._rule_based_rewrite(content, strength, preserve_terms)
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­
        """
        # åŒ¹é…ä¸­è‹±æ–‡å¥æœ«æ ‡ç‚¹
        pattern = r'([ã€‚ï¼ï¼Ÿï¼›.!?;])'
        parts = re.split(pattern, text)
        
        sentences = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and re.match(pattern, parts[i + 1]):
                sentences.append(parts[i] + parts[i + 1])
                i += 2
            else:
                if parts[i].strip():
                    sentences.append(parts[i])
                i += 1
        
        return sentences
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ (ä¼˜åŒ–ç‰ˆï¼šç»“åˆå­—ç¬¦çº§å’Œè¯çº§)
        """
        from difflib import SequenceMatcher
        
        # å­—ç¬¦çº§ç›¸ä¼¼åº¦
        char_sim = SequenceMatcher(None, text1, text2).ratio()
        
        # è¯çº§ç›¸ä¼¼åº¦ (ç®€å•åˆ†è¯)
        words1 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text1))
        words2 = set(re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text2))
        
        if words1 and words2:
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            word_sim = intersection / union if union > 0 else 0
        else:
            word_sim = char_sim
        
        # åŠ æƒå¹³å‡
        return 0.6 * char_sim + 0.4 * word_sim
    
    def _identify_changes(self, original: str, processed: str) -> List[str]:
        """
        è¯†åˆ«ä¸»è¦å˜åŒ–
        """
        changes = []
        
        # å­—æ•°å˜åŒ–
        orig_len = len(original)
        proc_len = len(processed)
        len_ratio = proc_len / orig_len if orig_len > 0 else 1
        
        if len_ratio > 1.2:
            changes.append(f"é€‚å½“æ‰©å±•äº†å†…å®¹è¡¨è¿°ï¼ˆå¢åŠ çº¦{int((len_ratio-1)*100)}%ï¼‰")
        elif len_ratio > 1.05:
            changes.append("ç•¥å¾®æ‰©å±•äº†éƒ¨åˆ†è¡¨è¿°")
        elif len_ratio < 0.8:
            changes.append(f"ç²¾ç®€äº†å†—ä½™è¡¨è¾¾ï¼ˆå‡å°‘çº¦{int((1-len_ratio)*100)}%ï¼‰")
        elif len_ratio < 0.95:
            changes.append("ç•¥å¾®ç²¾ç®€äº†éƒ¨åˆ†è¡¨è¿°")
        
        # å¥å­æ•°é‡å˜åŒ–
        orig_sentences = len(self._split_sentences(original))
        proc_sentences = len(self._split_sentences(processed))
        
        if proc_sentences > orig_sentences * 1.3:
            changes.append("æ‹†åˆ†äº†é•¿å¥ï¼Œå¢åŠ äº†å¥å­æ•°é‡")
        elif proc_sentences < orig_sentences * 0.7:
            changes.append("åˆå¹¶äº†ç›¸å…³å¥å­ï¼Œå¢å¼ºè¿è´¯æ€§")
        
        # æ£€æŸ¥åŒä¹‰è¯æ›¿æ¢
        replaced_count = 0
        for word in self.SYNONYM_DICT.keys():
            if word in original and word not in processed:
                replaced_count += 1
        
        if replaced_count >= 3:
            changes.append(f"è¿›è¡Œäº†{replaced_count}å¤„åŒä¹‰è¯æ›¿æ¢")
        elif replaced_count > 0:
            changes.append("è¿›è¡Œäº†åŒä¹‰è¯æ›¿æ¢")
        
        # æ£€æŸ¥å¥å¼å˜åŒ–
        for pattern, _ in self.SENTENCE_PATTERNS:
            if re.search(pattern, original) and not re.search(pattern, processed):
                changes.append("è°ƒæ•´äº†å¥å¼ç»“æ„")
                break
        
        if not changes:
            changes.append("è°ƒæ•´äº†è¯æ±‡å’Œè¡¨è¾¾æ–¹å¼")
        
        return changes[:MAX_CHANGES_TO_REPORT]
    
    def get_dedup_report(self, result: DedupResult) -> str:
        """
        ç”Ÿæˆé™é‡æŠ¥å‘Š
        """
        lines = []
        lines.append("## ğŸ“Š é™é‡å¤„ç†æŠ¥å‘Š\n")
        
        # ç›¸ä¼¼åº¦å˜åŒ–
        reduction = (result.similarity_before - result.similarity_after) * 100
        lines.append("### ç›¸ä¼¼åº¦å˜åŒ–")
        lines.append(f"- å¤„ç†å‰ç›¸ä¼¼åº¦ï¼š{result.similarity_before * 100:.1f}%")
        lines.append(f"- å¤„ç†åç›¸ä¼¼åº¦ï¼š{result.similarity_after * 100:.1f}%")
        lines.append(f"- **é™é‡å¹…åº¦ï¼š{reduction:.1f}%**")
        lines.append(f"- å¤„ç†å¥å­æ•°ï¼š{result.sentences_processed}")
        lines.append("")
        
        # ä¿ç•™æœ¯è¯­
        if result.preserved_terms:
            lines.append("### ä¿ç•™çš„ä¸“ä¸šæœ¯è¯­")
            lines.append(", ".join(result.preserved_terms[:10]))
            if len(result.preserved_terms) > 10:
                lines.append(f"...ç­‰å…±{len(result.preserved_terms)}ä¸ªæœ¯è¯­")
            lines.append("")
        
        # ä¸»è¦å˜åŒ–
        lines.append("### ä¸»è¦å˜åŒ–")
        for change in result.changes:
            lines.append(f"- {change}")
        
        return "\n".join(lines)
